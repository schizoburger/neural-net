{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "\n",
    "    def __init__(self, layers_size,\\\n",
    "                 do_random_seed=True, random_seed=1):\n",
    "        self.layers_size = layers_size\n",
    "        if do_random_seed == True:\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.init_biases()\n",
    "        \n",
    "        self.regularize = False\n",
    "        self.do_l2 = False\n",
    "        self.do_dropout = False\n",
    "        self.test_time = False\n",
    "    \n",
    "    def init_biases(self):\n",
    "        self.biases = [self.get_biases(self.layers_size[i]) for i in range(1, len(self.layers_size))]\n",
    "    \n",
    "    def get_biases(self, output_size):\n",
    "        return np.zeros((1,output_size))\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.weights = [self.get_weights(self.layers_size[i-1],self.layers_size[i]) for i in range(1, len(self.layers_size))]\n",
    "    \n",
    "    def get_weights(self, input_size, output_size):\n",
    "        return np.random.randn(input_size,output_size)*np.sqrt(2.0/input_size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        stable_sigmoid = np.vectorize(Neural_Network.__stable_sigmoid_function)\n",
    "        return stable_sigmoid(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __stable_sigmoid_function(x):\n",
    "        \"Numerically-stable sigmoid function.\"\n",
    "        if x >= 0:\n",
    "            z = np.exp(-x)\n",
    "            return 1/(1+z)\n",
    "        else:\n",
    "            z = np.exp(x)\n",
    "            return z/(1+z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        return np.exp(x-np.max(x))/np.sum(np.exp(x-np.max(x)), axis=1, keepdims=True)\n",
    "    \n",
    "    def cross_entropy(self, yHat, y):\n",
    "        return -1/yHat.shape[0]*np.sum((y*np.log(yHat)+\\\n",
    "                (1-y)*np.log(1-yHat)))+\\\n",
    "                self.__loss_regularization_term()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.training_size = x.shape[0]\n",
    "        self.activations = [x]\n",
    "        for w, b, activation in zip(self.weights, self.biases, self.activations):\n",
    "            self.activations.append(self.sigmoid(np.dot(activation, w) + b))\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        deltas = [self.activations[-1]-y]\n",
    "        dJdWeights = [np.dot(self.activations[-2].T,deltas[-1])]\n",
    "        dJdBiases = [np.sum(deltas[-1],axis=0)]\n",
    "        i=len(self.weights)-2\n",
    "        while i>=0:\n",
    "            deltas.append(np.dot(deltas[-1],self.weights[i+1].T)*(self.activations[i+1]*(1-self.activations[i+1])))\n",
    "            dJdWeights.append(np.dot(self.activations[i].T,deltas[-1]))\n",
    "            dJdBiases.append(np.sum(deltas[-1],axis=0))\n",
    "            i = i - 1\n",
    "        # tuple(map(operator.add, (dJdW1, dJdW2), self.__derivative_regularization_term()))\n",
    "        return deltas, dJdWeights, dJdBiases\n",
    "\n",
    "    def learn_using_stochastic_gradient_descent(self, learning_rate, x, y, mini_batch_size):\n",
    "        combined_train_array = np.append(x,y,axis=1)\n",
    "        random.shuffle(combined_train_array)\n",
    "        x_train = combined_train_array[:,:-y.shape[1]]\n",
    "        y_train = combined_train_array[:,-y.shape[1]:]\n",
    "        for k in range(0, x_train.shape[0], mini_batch_size):\n",
    "            self.forward(x_train[k:k+mini_batch_size])\n",
    "            deltas, dJdWeights, dJdBiases = self.backprop(x_train[k:k+mini_batch_size],y_train[k:k+mini_batch_size])\n",
    "            i=-1\n",
    "            for j in range(len(self.weights)):\n",
    "                self.weights[j] = self.weights[j] - learning_rate*dJdWeights[i]\n",
    "                i = i - 1\n",
    "            i=-1\n",
    "            for j in range(len(self.biases)):\n",
    "                self.biases[j] = self.biases[j] - learning_rate*dJdBiases[i]\n",
    "                i = i - 1\n",
    "    \n",
    "    def dropout(self):\n",
    "        if self.do_dropout==True:\n",
    "            if self.test_time==False:\n",
    "                self.dropout_hidden_mask = (np.random.rand(self.hidden_size)<self.dropout_rate)\n",
    "                self.a2 *= self.dropout_hidden_mask\n",
    "            else:\n",
    "                self.W2 *= self.dropout_rate\n",
    "        \n",
    "    def apply_dropout(self, dropout_rate):\n",
    "        self.do_dropout=True\n",
    "        self.dropout_rate=1-dropout_rate\n",
    "    \n",
    "    def apply_l2_regularization(self, regularization_rate):\n",
    "        self.regularize=True\n",
    "        self.do_l2=True\n",
    "        self.regularization_rate=regularization_rate\n",
    "    \n",
    "    def __derivative_regularization_term(self):\n",
    "        if self.regularize==False:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            if self.do_l2==True:\n",
    "                return self.regularization_rate/self.training_size*self.W1, self.regularization_rate/self.training_size*self.W2\n",
    "            \n",
    "    def __loss_regularization_term(self):\n",
    "        if self.regularize==False:\n",
    "            return 0\n",
    "        else:\n",
    "            if self.do_l2==True:\n",
    "                return self.regularization_rate/(2*self.training_size)*\\\n",
    "            (sum(np.linalg.norm(w)**2 for w in self.W1)+\\\n",
    "             sum(np.linalg.norm(w)**2 for w in self.W2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(yHat_test, y_test, threshold=None):\n",
    "        multiple_outputs = y_test.shape[1] > 1\n",
    "        correctly_classified=0\n",
    "        for i in range(y_test.shape[0]):\n",
    "            if multiple_outputs == True:\n",
    "                if np.argmax(yHat_test[i])==np.argmax(y_test[i]):\n",
    "                    correctly_classified+=1\n",
    "            else:\n",
    "                prediction = yHat_test[i] > threshold\n",
    "                if int(prediction)==int(y_test[i]):\n",
    "                    correctly_classified += 1\n",
    "        return correctly_classified/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
