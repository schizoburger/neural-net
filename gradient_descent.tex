\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\title{Gradient Descent}
\author{Michal Zmyslowski}
\date{September 2017}

\begin{document}
\maketitle
\begin{abstract}
Gradient descent is a popular first-order optimization algorithm in machine learning for minimizing a function. It looks really simple, but the reasoning for why it works is usually superficial. In the first section, we will use mathematical heuristics to show that it actually makes sense. In the second section, we will give the formal proof for its convergence together with the proof for the important lemma it relies on, which is usually omitted in the optimization textbooks.
\end{abstract}
\section{Heuristics}
The gradient descent is defined as follows,
$$\mathbf{x_{k+1}}:=\mathbf{x_{k}}-t\nabla f(\mathbf{x_k}).$$
In this section, we will show where the term $-t\nabla f(\mathbf{x_k})$ comes from and we will be content with approximations and saying that 1 is a small value.
\bigskip
\newline
Let $f:\mathbb{R}^n \to \mathbb{R}^n$ be a differentiable function and $\mathbf{x_{0}}. \in \mathbb{R}^n$. We want to 
\newline
find $\mathbf{x} \in \mathbb{R}^n$ such that $f(\mathbf{x_{0}}+\mathbf{x}) \leq f(\mathbf{x_{0}})$. By Taylor series,
\begin{equation}
f(\mathbf{x}) \approx f(\mathbf{x_{0}})+ \nabla f(\mathbf{x_{0}})^{\mathbf{T}}(\mathbf{x}-\mathbf{x_{0}})
\end{equation}
\noindent
is a good linear approximation of $f$ locally at $x_{0}$. From (1),

$$f(\mathbf{x_{0}} + \mathbf{x}) \approx f(\mathbf{x_{0}}) + \nabla f(\mathbf{x_{0}})^{\mathbf{T}}\mathbf{x}$$
\noindent
We want to find $\mathbf{x}$ such that (1) is still good approximation of $f$ in order to $f(\mathbf{x_{0}}+\mathbf{x}) \leq f(\mathbf{x_{0}})$. Therefore, we need to make $\|\mathbf{x}\|$ small. As a matter of convenience, let $\mathbf{x}$ be a unit vector, i.e. $\|\mathbf{x}\|=1$. Thus,

$$f(\mathbf{x_{0}} + \mathbf{x}) \approx f(\mathbf{x_{0}}) + \nabla f(\mathbf{x_{0}})^{\mathbf{T}}\mathbf{x}=$$

$$f(\mathbf{x_{0}}) + \| \nabla f(\mathbf{x_{0}})\|\|\mathbf{x}\|cos\theta$$
\noindent
where $\theta$ is an angle between $\mathbf{x_0}$ and $\nabla f(\mathbf{x_0})$ and $\|\centerdot\|$ is the Euclidean norm. From assumption that $\|\mathbf{x}\|=1$,

$$f(\mathbf{x_{0}} + \mathbf{x}) \approx f(\mathbf{x_{0}}) + \| \nabla f(\mathbf{x_{0}})\|cos\theta$$
\noindent
Since $f(\mathbf{x_0})$ and $\nabla f(\mathbf{x_{0}})$ are constants and $\| \nabla f(\mathbf{x_{0}})\| \geq 0$, we want to find $\mathbf{x}$ such that $cos\theta=-1$ to decrease the function maximally. Let $\mathbf{x}=- \frac{\nabla f(\mathbf{x_0})}{\|\nabla f(\mathbf{x_0})\|}$ and we see that 
$\|x\|=\|- \frac{\nabla f(\mathbf{x_0})}{\|\nabla f(\mathbf{x_0})\|}\|=\frac{\|\nabla f(\mathbf{x_0})\|}{\|\nabla f(\mathbf{x_0})\|}=1$. Now,

$$\nabla f(\mathbf{x_{0}})^{\mathbf{T}}(- \frac{\nabla f(\mathbf{x_0})}{\|\nabla f(\mathbf{x_0})\|})=\| \nabla f(\mathbf{x_{0}})\|cos\theta$$

$$\Leftrightarrow -\nabla f(\mathbf{x_{0}})^{\mathbf{T}}\nabla f(\mathbf{x_{0}})=(\sqrt{\nabla f(\mathbf{x_{0}})^{\mathbf{T}}\nabla f(\mathbf{x_{0}})})^2cos\theta $$

$$\Leftrightarrow cos \theta = -1$$


\section{Convergence proof}
In this section, we will show the formal proof for the convergence of the gradient descent algorithm. Although before we begin, we will prove a useful lemma.
\begin{lemma}
If $f$ is differentiable and $f'$ is Lipschitz continuous with constant L, then $$|f(\mathbf{x})-f(\mathbf{y})-\nabla f(\mathbf{y})^\mathbf{T}(\mathbf{x}-\mathbf{y})|\leq\frac{L}{2}\|\mathbf{x}-\mathbf{y}\|^2$$
\end{lemma}

\begin{proof}
Let $g(t):=f(\mathbf{y}+t(\mathbf{x}-\mathbf{y}))$, then $g(0)=f(\mathbf{y})$, $g(1)=f(\mathbf{x})$ and
\newline
$\frac{dg(t)}{dt}=\nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T}(\mathbf{x}-\mathbf{y})$. Now we have,
$\int\limits_0^1 \frac{dg(t)}{dt} \ dt=g(1)-g(0)$. Therefore,
$$|f(\mathbf{x})-f(\mathbf{y})-\nabla f(\mathbf{y})^\mathbf{T}(\mathbf{x}-\mathbf{y})|=$$
$$|g(1)-g(0)-\nabla f(\mathbf{y})^\mathbf{T}(\mathbf{x}-\mathbf{y})|=$$
$$| \int\limits_0^1 \frac{dg(t)}{dt} \ dt-\nabla f(\mathbf{y})^\mathbf{T}(\mathbf{x}-\mathbf{y})|=$$
$$| \int\limits_0^1 \nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T}(\mathbf{x}-\mathbf{y}) \ dt-\nabla f(\mathbf{y})^\mathbf{T}(\mathbf{x}-\mathbf{y})|=$$
$$| \int\limits_0^1 \nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T}(\mathbf{x}-\mathbf{y}) \ dt-\int\limits_0^1 \nabla f(\mathbf{y})^\mathbf{T}(\mathbf{x}-\mathbf{y})\ dt|\leq$$
$$\int\limits_0^1 |\nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T}(\mathbf{x}-\mathbf{y}) - \nabla f(\mathbf{y})^\mathbf{T}(\mathbf{x}-\mathbf{y})|\ dt=$$
$$\int\limits_0^1 |(\nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T} - \nabla f(\mathbf{y})^\mathbf{T})(\mathbf{x}-\mathbf{y})|\ dt$$

By Cauchyâ€“Schwarz inequality:

$$\int\limits_0^1 |(\nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T} - \nabla f(\mathbf{y})^\mathbf{T})(\mathbf{x}-\mathbf{y})|\ dt \leq$$

$$\int\limits_0^1 \|\nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T} - \nabla f(\mathbf{y})^\mathbf{T}\|\|(\mathbf{x}-\mathbf{y})\| dt=$$

$$ \|(\mathbf{x}-\mathbf{y})\| \int\limits_0^1 \|\nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T} - \nabla f(\mathbf{y})^\mathbf{T}\|dt$$

Since $f'$ is Lipschitz continuous with constant L, we get:

$$ \|(\mathbf{x}-\mathbf{y})\| \int\limits_0^1 \|\nabla f(\mathbf{y}-t(\mathbf{x}-\mathbf{y}))^\mathbf{T} - \nabla f(\mathbf{y})^\mathbf{T}\|dt \leq$$

$$ \|(\mathbf{x}-\mathbf{y})\| \int\limits_0^1\frac{L}{2}\|(\mathbf{y}-t(\mathbf{x}-\mathbf{y})-\mathbf{y})\|\ dt=$$

$$ \|(\mathbf{x}-\mathbf{y})\| \int\limits_0^1\frac{L}{2}\|(\mathbf{x}-\mathbf{y})\|t\ dt=$$

$$\frac{L}{2}\|(\mathbf{x}-\mathbf{y})\|^2$$

\end{proof}

Functions satisfying $f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{y})^\mathbf{T} (\mathbf{x}-\mathbf{y}) + \frac{L}{2} \|\mathbf{x}-\mathbf{y}\|^2$ with some constant L are called strongly convex.

\begin{theorem}
Let $f$ be differentiable, $f'$ be Lipschitz continuous with constant L and $minf(x)>-\infty$. Then the gradient descent algorithm with a constant step size $t<\frac{2}{L}$ will converge to a stationary point, i.e. $\nabla f(\mathbf{x})=0$.
\end{theorem}

\begin{proof}
The gradient descent is defined as follows,
\begin{equation}
\mathbf{x_{k+1}}:=\mathbf{x_{k}}-t\nabla f(\mathbf{x_k}).
\end{equation}
\newline
By Lemma 1,

$$f(\mathbf{x_{k+1}}) \leq f(\mathbf{x_{k}})+ \nabla f(\mathbf{x_{k}})(\mathbf{x_{k+1}}-\mathbf{x_{k+1}})+ \frac{L}{2}\|(\mathbf{x}-\mathbf{y})\|^2$$
From (1),
$$\mathbf{x_{k+1}}-\mathbf{x_{k}}=-t\nabla f(\mathbf{x_k})$$
Therefore,
$$f(\mathbf{x_{k}})+ \nabla f(\mathbf{x_{k}})(\mathbf{x_{k+1}}-\mathbf{x_{k+1}})+ \frac{L}{2}\|(\mathbf{x}-\mathbf{y})\|^2=$$

$$f(\mathbf{x_{k}})+ \nabla f(\mathbf{x_{k}})(-t\nabla f(\mathbf{x_k}))+ \frac{L}{2}\|(-t\nabla f(\mathbf{x_k}))\|^2=$$

$$f(\mathbf{x_{k}})-t\|\nabla f(\mathbf{x_k})\|^2+ \frac{Lt^2}{2}\|\nabla f(\mathbf{x_k})\|^2=$$

$$f(\mathbf{x_{k}})-t(1 - \frac{Lt}{2}) \| \nabla f(\mathbf{x_k})\|^2$$

Thus,

$$f(\mathbf{x_{k+1}}) \leq f(\mathbf{x_{k}})-t(1 - \frac{Lt}{2}) \| \nabla f(\mathbf{x_k})\|^2$$

$$\Rightarrow f(\mathbf{x_{k+1}}) - f(\mathbf{x_{k}}) \leq -t(1 - \frac{Lt}{2}) \| \nabla f(\mathbf{x_k})\|^2$$

Rearranging terms and noting that $-t(1 - \frac{Lt}{2}) < 0$, since we assumed that $t<\frac{2}{L}$, we get

$$\| \nabla f(\mathbf{x_k})\|^2 \leq \frac{f(\mathbf{x_{k}}) - f(\mathbf{x_{k+1}})}{t(1 - \frac{Lt}{2})}$$

$$\Rightarrow\sum_{k=0}^{N} \| \nabla f(\mathbf{x_k})\|^2 \leq \frac{1}{t(1 - \frac{Lt}{2})} \sum_{k=0}^{N} f(\mathbf{x_{k}}) - f(\mathbf{x_{k+1}})$$

We note that,

$$\frac{1}{t(1 - \frac{Lt}{2})} \sum_{k=0}^{N} f(\mathbf{x_{k}}) - f(\mathbf{x_{k+1}})=\frac{f(\mathbf{x_{0}}) - f(\mathbf{x_{N}})}{t(1 - \frac{Lt}{2})}$$

$$\frac{f(\mathbf{x_{0}}) - f(\mathbf{x_{N}})}{t(1 - \frac{Lt}{2})} \leq \frac{f(\mathbf{x_{0}}) - minf(\mathbf{x})}{t(1 - \frac{Lt}{2})}$$

Therefore,

$$\lim_{N\to\infty} \sum_{k=0}^{N} \| \nabla f(\mathbf{x_k})\|^2 < \infty$$

$$\Rightarrow\lim_{k\to\infty} \| \nabla f(\mathbf{x_k})\|^2 = 0$$

$$\Rightarrow\lim_{k\to\infty} \nabla f(\mathbf{x_k}) = 0$$
\end{proof}
\end{document}
