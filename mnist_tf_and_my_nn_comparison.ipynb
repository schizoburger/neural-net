{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "import neural_network as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.random_normal(shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0., shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, W1, W2, b1, b2):\n",
    "    h1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n",
    "    y = tf.matmul(h1,W2) + b2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = weight_variable([784, 100])\n",
    "b1 = bias_variable([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = weight_variable([100, 10])\n",
    "b2 = bias_variable([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = forward(x, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.3\n",
      "step 6000, training accuracy 0.9\n",
      "step 12000, training accuracy 0.9\n",
      "step 18000, training accuracy 0.9\n",
      "step 24000, training accuracy 0.9\n",
      "step 30000, training accuracy 0.7\n",
      "step 36000, training accuracy 0.8\n",
      "step 42000, training accuracy 1\n",
      "step 48000, training accuracy 1\n",
      "step 54000, training accuracy 0.7\n",
      "step 60000, training accuracy 1\n",
      "step 66000, training accuracy 1\n",
      "step 72000, training accuracy 1\n",
      "step 78000, training accuracy 0.9\n",
      "step 84000, training accuracy 0.8\n",
      "step 90000, training accuracy 1\n",
      "step 96000, training accuracy 0.9\n",
      "step 102000, training accuracy 0.9\n",
      "step 108000, training accuracy 0.8\n",
      "step 114000, training accuracy 0.8\n",
      "step 120000, training accuracy 0.9\n",
      "step 126000, training accuracy 1\n",
      "step 132000, training accuracy 1\n",
      "step 138000, training accuracy 1\n",
      "step 144000, training accuracy 0.9\n",
      "step 150000, training accuracy 0.9\n",
      "step 156000, training accuracy 0.9\n",
      "step 162000, training accuracy 1\n",
      "step 168000, training accuracy 0.8\n",
      "step 174000, training accuracy 0.6\n",
      "\n",
      "\n",
      "--- 216.24643087387085 seconds ---\n",
      "cv accuracy 0.9322\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    for i in range(180000):\n",
    "        batch = mnist.train.next_batch(10)\n",
    "        if i % 6000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1]})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    print(\"\\n\\n--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print('cv accuracy %g' % accuracy.eval(feed_dict={\n",
    "        x: mnist.validation.images, y_: mnist.validation.labels}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_cv = mnist.validation.images\n",
    "y_cv = mnist.validation.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = nn.Neural_Network(dimensionality=x_train.shape[1],hidden_size=100,\\\n",
    "        output_size=10,learning_rate=0.1,\\\n",
    "        dropout_hidden_rate=0.5,do_dropout=False,\\\n",
    "        error_function='cross_entropy',do_regularize=False,\\\n",
    "        regularization_rate=1,add_bias=True,use_nesterov_momentum=False,\\\n",
    "        momentum_rate=0.9,do_random_seed=False,random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 loss  0.269274357824\n",
      "\n",
      "\n",
      "--- 5.428372859954834 seconds ---\n",
      "Accuracy on CV with stochastic gradient descent:  0.9602\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(1):\n",
    "    NN.learn_using_stochastic_gradient_descent(x=x_train,y=y_train,\\\n",
    "    mini_batch_size=10,current_epoch=i, clip=True,print_loss=True)\n",
    "\n",
    "print(\"\\n\\n--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Accuracy on CV with stochastic gradient descent: ', NN.accuracy(x_cv,y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
